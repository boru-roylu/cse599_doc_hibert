{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import copydf\n",
    "import datasets\n",
    "import decouple\n",
    "import einops\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers as tfs\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "import data_collator\n",
    "import modeling_bert\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiBert (choose one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_hierarchical = True\n",
    "output_dir = './project_dir/cse599/hibert'\n",
    "data_path = './project_dir/cse599/data/preprocessed_data.jsonl'\n",
    "pretrained_hibert_model_path = './project_dir/cse599/hibert/checkpoint-500'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert (choose one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_hierarchical = False\n",
    "output_dir = './project_dir/cse599/bert'\n",
    "data_path = './project_dir/cse599/data/preprocessed_abstracts.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_config_path = './ctx-hibert-absolute-pos-config.json'\n",
    "model_path = './project_dir/pretrained_models/allenai/specter'\n",
    "emb_dir = os.path.join(output_dir, 'embs')\n",
    "os.makedirs(emb_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2507f9f70be6dc2f\n",
      "Reusing dataset json (/homes/roylu/.cache/huggingface/datasets/json/default-2507f9f70be6dc2f/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8619329b3ddd49c18f608602c730704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /homes/roylu/.cache/huggingface/datasets/json/default-2507f9f70be6dc2f/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-277494dfe93753b5.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tfs.AutoTokenizer.from_pretrained(model_path)\n",
    "doc_ds = datasets.load_dataset('json', data_files=data_path)\n",
    "preprocess = utils.get_preprocess_fn(tokenizer, 128)\n",
    "ds = doc_ds.map(preprocess, batched=True)\n",
    "ds.set_format('torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = data_collator.DataCollatorForWholeWordMaskAndWholeSentenceMask(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm_probability=0.15,\n",
    "    msm_probability=0.15,\n",
    "    max_num_turns=32,\n",
    "    mask_whole_sentence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train (Hibert)\n",
    "skip if using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "loading configuration file ./ctx-hibert-absolute-pos-config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./ctx-hibert-absolute-pos-config.json\",\n",
      "  \"add_absolute_position_embeddings\": true,\n",
      "  \"add_ctx_pooled_output_to_tokens\": true,\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 300,\n",
      "  \"initializer_range\": 0.005,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ./project_dir/pretrained_models/allenai/specter/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./project_dir/pretrained_models/allenai/specter\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 31116\n",
      "}\n",
      "\n",
      "loading weights file ./project_dir/pretrained_models/allenai/specter/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing CustomBertForMaskedLM.\n",
      "\n",
      "Some weights of CustomBertForMaskedLM were not initialized from the model checkpoint at ./project_dir/pretrained_models/allenai/specter and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "args = tfs.TrainingArguments(\n",
    "    num_train_epochs=200,\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=16,\n",
    "    per_device_train_batch_size=1,\n",
    "    output_dir=output_dir)\n",
    "model_init_fn = functools.partial(\n",
    "    utils.ctx_hibert_model_init,\n",
    "    tokenizer=tokenizer,\n",
    "    model_path=model_path,\n",
    "    coordinator_config_path=coordinator_config_path)\n",
    "trainer = tfs.Trainer(\n",
    "    args=args,\n",
    "    model_init=model_init_fn,\n",
    "    train_dataset=ds['train'],\n",
    "    data_collator=dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./ctx-hibert-absolute-pos-config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./ctx-hibert-absolute-pos-config.json\",\n",
      "  \"add_absolute_position_embeddings\": true,\n",
      "  \"add_ctx_pooled_output_to_tokens\": true,\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 300,\n",
      "  \"initializer_range\": 0.005,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ./project_dir/pretrained_models/allenai/specter/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./project_dir/pretrained_models/allenai/specter\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 31116\n",
      "}\n",
      "\n",
      "loading weights file ./project_dir/pretrained_models/allenai/specter/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing CustomBertForMaskedLM.\n",
      "\n",
      "Some weights of CustomBertForMaskedLM were not initialized from the model checkpoint at ./project_dir/pretrained_models/allenai/specter and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `HierarchicalBertForMaskedLM.forward` and have been ignored: num_sections, document. If num_sections, document are not expected by `HierarchicalBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/g/ssli/transitory/roylu/miniconda3/envs/conv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 78\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 400\n",
      "/g/ssli/transitory/roylu/miniconda3/envs/conv/lib/python3.8/site-packages/transformers/modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/g/ssli/transitory/roylu/miniconda3/envs/conv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28/400 06:26 < 1:32:08, 0.07 it/s, Epoch 13.41/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer.model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = 0\n",
    "coordinator_config = tfs.AutoConfig.from_pretrained(coordinator_config_path)\n",
    "model_config = tfs.AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "if is_hierarchical:\n",
    "    model = modeling_bert.HierarchicalBertForMaskedLM.from_pretrained(\n",
    "        pretrained_hibert_model_path, config=model_config, coordinator_config=coordinator_config)\n",
    "else:\n",
    "    model = modeling_bert.HierarchicalBertForMaskedLM(\n",
    "        config=model_config, coordinator_config=coordinator_config)\n",
    "    bert = tfs.AutoModel.from_pretrained(model_path)\n",
    "    model.hibert.bert = bert\n",
    "model = model.hibert\n",
    "\n",
    "_ = model.cuda(cuda_device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracts embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing training set.')\n",
    "train_output = utils.get_turn_embeddings(\n",
    "    model, ds['train'], dc, slide=None, cuda_device=cuda_device)\n",
    "utils.save_embeddings(emb_dir, 'train', train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads extraced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = utils.load_embeddings(emb_dir, 'train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('conv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c272255b56e5b7e28e39cae529fe625ab1b57af2d64c90b04b0d0930b7089a36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
